import json
import jsonlines
import csv
import sqlglot
import re
import datetime
import pandas as pd
import sqlite3, random
import string
import glob
import openai
import os
import jsonlines
import psycopg2
from func_timeout import func_timeout, FunctionTimedOut
from dotenv import load_dotenv

from typing import Tuple

test_dbs, train_dbs = None, None
load_dotenv()


def delete_qa_files(directory):
    
    for filename in os.listdir(directory):
        # Construct the full path
        full_path = os.path.join(directory, filename)
        
        # Check if it's a file
        if os.path.isfile(full_path):
            # If it's a file, delete it
            os.remove(full_path)
    
    print("Cleaned up cache files...")
    
            
def open_csv_as_string(filename):
    """
    Open a CSV file and read its contents as a string.
    """
    lines = []
    with open(filename, 'r') as file:
        # Read and print each line
        for line in file:
            lines.append(line.strip())
    # join
    return '\n'.join(lines)
    
def read_jsonl(file_path):
    """
    Read a JSONL file and return the data as a list.
    """
    data = []
    with jsonlines.open(file_path) as reader:
        for line in reader:
            data.append(line)
    return data


def write_jsonl(file_path, data):
    """
    Write data to a JSONL file.
    """
    with jsonlines.open(file_path, 'w') as writer:
        for d in data:
            writer.write(d)

def validate_sql_generation(sentence):
    """
    Validate the SQL query structure generated by the model. (by parsing through a SQL parser)
    """
    print("Validating SQL: ", sentence)
    try:
        s= sqlglot.transpile(sentence)
        
    except Exception as e:
        print(e)
        print(f"Error in parsing SQL: {sentence}")
        return 0,sentence
    return 1,s

def  preprocess_output(output, system_prompt, user_prompt, flag='', test_against_db=False, db_name='defog_data_private'):
    """
    Preprocess the output from the GPT model by removing any leading/trailing whitespaces and newlines.
    
    :param output: The output from the GPT model
    :return: The preprocessed output

    """
    # pattern = r"(?:User Question|Question):\s*(.*?)\n (?:SQL|SQL Code):\s*```(?:sql)?\s*(.*?)\s*```"
    pattern = r"(?:User Question|Question):\s*(.*?)\n(?:SQL:|SQL Code:|SQL code:)\s*```(?:sql)?\s*(.*?)\s*```"

    match = re.findall(pattern, output, re.DOTALL)
    qa_pairs = []

    lines = output.split('#')
    count_valid = 0
    for question, sql_query in match:
        # print(question)
        # print(sql_query)

        val,parsed_sql = validate_sql_generation(str(sql_query))
        if val == True:
            count_valid += 1
        # val = validate_sql_generation(sql_query) # sql parsing validation

        if test_against_db: # validate the query against the test db
            # validate the query against the test db
            print("Validating SQL query against test db...")
            result,_ = test_valid_against_db(sql_query, db_name)
            if result != 1:
                val = 0
                count_valid -= 1
            elif result == 1:
                val = 1
            # print(" --- ")
            pass
        qa_pair = [{'question': question, 'sql': sql_query, 'valid': val }]
        qa_pairs.append(qa_pair)

    print(" --- ")
    print("Fraction of valid SQL queries: ")
    try:
        print(count_valid/len(match))
        print(" --- ")
    except ZeroDivisionError:
        print("error matching  SQL queries...")
        print(" --- ")

    qa_pairs = [{'system_prompt': system_prompt, 'user_prompt': user_prompt}] + qa_pairs
    time_stamp = datetime.datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
    write_jsonl(f'./qa_collection/qa_pairs_{time_stamp}_{flag}.jsonl', qa_pairs)
    return qa_pairs


def dtype_mapping(data_type):
    """_summary_

    Args:
        data_type (string): maps to relevant SQL data type
    Returns:
        string: SQL data type
    """
    return {
        'character varying': 'TEXT',
        'integer': 'INTEGER',
        'date': 'DATE',
        'numeric': 'NUMERIC',
        'bigint': 'BIGINT',
        'boolean': 'BOOLEAN',
        'real': 'FLOAT',
    }.get(data_type) 


def create_table_from_df(df):
    """
    Create a SQL table from a pandas DataFrame. 
    Creates a dummy table to test the SQL queries generated by the model.
    """
    sql_statements = []
    for _, row in df.iterrows():
        table_name = row['table_name']
        column_name = row['column_name']
        data_type = dtype_mapping(row['data_type'])
        column_description = row['column_description']
        
        column_statement = f"{column_name} {data_type}"

        # Check for possible values in the column description
        if ": " in column_description:
            possible_values_part = column_description.split(": ")[1]
            # Handle cases where possible values are listed and replace '-' with '_'
            possible_values = [value.strip().replace('-', '_') for value in possible_values_part.split(", ")]
            possible_values_formatted = ', '.join(f"'{value}'" for value in possible_values)
            column_statement += f" CHECK ({column_name} IN ({possible_values_formatted}))"
        
        sql_statements.append(column_statement)
    
    create_table_sql = f"CREATE TABLE {table_name} (\n    " + ",\n    ".join(sql_statements) + "\n);"
    print(create_table_sql)
    return create_table_sql

def run_sql_query(sql_query):
    """
    run the sql query against dummy db and return the cursor object
    """
    # Connect to the SQLite in-memory database
    conn = sqlite3.connect(':memory:')
    # Create a cursor object using the cursor() method
    cur = conn.cursor() 
    cur.execute(sql_query)
    # Commit the changes to the database
    conn.commit()
    cur.execute("SELECT * FROM temp_achive_lp_sapling_combined_activations_ai;")

    return cur

def random_string(prefix='', length=10, suffix=''):
    """ helper function to generate random string for TEXT fields
    """

    letters = string.ascii_letters
    return f"{prefix}{''.join(random.choice(letters) for _ in range(length))}{suffix}"

# Function to generate a random date between two dates
def random_date(start_date, end_date):
    time_between_dates = end_date - start_date
    days_between_dates = time_between_dates.days
    random_number_of_days = random.randrange(days_between_dates)
    return start_date + datetime.timedelta(days=random_number_of_days)

# Function to generate random data for the given table structure
def generate_random_data_for_table(table_name, num_records):
    random_data = []
    start_date = datetime.date(2015, 1, 1)
    end_date = datetime.date(2023, 12, 31)
    for _ in range(num_records):
        record = {
            'unique_id': random_string(length=8),
            'copyright_year': random.randint(1990, 2023),
            'year': random.randint(1990, 2023),
            'month': random.randint(1, 12),
            'purchased_from_studentstore': random.choice(['No', 'Yes_Purchase']),
            'inclusive_access': random.choice(['No', 'Yes']),
            'has_used_access_code': random.choice(['No', 'Yes']),
            'student_subscription_status': random.choice(['Active Access', 'Expired Access']),
            'product_status': random.choice(['REB', 'NAB', 'OP']),
            'account_manager_territory': random_string(),
            'acquisition_source': random_string(),
            'division': random_string(),
            'territory': random_string(),
            'region': random.choice(['Northeast Region', 'Southern Region', 'Rocky Mountain Region', 'Central Region']),
            'product_publisher': random_string(),
            'sfdc_state': random_string(),
            'discipline': random_string(),
            'category': random_string(),
            'product_sub_discipline': random_string(),
            'region_territory': random_string(),
            'instructor_email': random_string(suffix='@example.com', length=5),
            'instructor_name': random_string(length=15),
            'profit_center_desc': random_string(),
            'hs_col': random_string(),
            'author_display_name': random_string(),
            'project_title': random_string(),
            'platform': random_string(),
            'school': random_string(),
            'sfdc_country': random_string(),
            'sfdc_city': random_string(),
            'sfdc_account_name': random_string(),
            'paid_unpaid': random.choice(['Paid', 'Unpaid']),
            'course_name': random_string(),
            'course': random_string(),
            'course_id': random_string(length=8),
            'product_isbn': random_string(length=13),
            'isbn': random_string(length=13),
            'subscription_created_date': random_date(start_date, end_date),
            'subs_start_date': random_date(start_date, end_date),
            'subs_end_date': random_date(start_date, end_date),
            'return_request_date': random_date(start_date, end_date),
            'student_subscription_expiration_date': random_date(start_date, end_date),
            'conversion_date': random_date(start_date, end_date),
            'course_end_date': random_date(start_date, end_date),
            'course_start_date': random_date(start_date, end_date),
            'course_created_date': random_date(start_date, end_date),
            'access_duration': round(random.uniform(0.0, 100.0), 2),
            'access_duration_derived': random.randint(1, 1000),
            'us_net_price': round(random.uniform(0.0, 1000.0), 2),
            'us_list_price': round(random.uniform(0.0, 1000.0), 2),
            'us_consumer_price': round(random.uniform(0.0, 1000.0), 2),
            'can_net_price': round(random.uniform(0.0, 1000.0), 2),
            'can_list_price': round(random.uniform(0.0, 1000.0), 2),
            'can_consumer_price': round(random.uniform(0.0, 1000.0), 2),
        }
        random_data.append(record)
    return random_data



def insert_random_data(cur, table_name, records):
    """
    Insert random data into a given table.
    """

    for i in range(len(records)):
        cur.execute(f"""
            INSERT INTO {table_name} (
                unique_id,
                copyright_year,
                year,
                month,
                purchased_from_studentstore,
                inclusive_access,
                has_used_access_code,
                student_subscription_status,
                product_status,
                account_manager_territory,
                acquisition_source,
                division,
                territory,
                region,
                product_publisher,
                sfdc_state,
                discipline,
                category,
                product_sub_discipline,
                region_territory,
                instructor_email,
                instructor_name,
                profit_center_desc,
                hs_col,
                author_display_name,
                project_title,
                platform,
                school,
                sfdc_country,
                sfdc_city,
                sfdc_account_name,
                paid_unpaid,
                course_name,
                course,
                course_id,
                product_isbn,
                isbn,
                subscription_created_date,
                subs_start_date,
                subs_end_date,
                return_request_date,
                student_subscription_expiration_date,
                conversion_date,
                course_end_date,
                course_start_date,
                course_created_date,
                access_duration,
                access_duration_derived,
                us_net_price,
                us_list_price,
                us_consumer_price,
                can_net_price,
                can_list_price,
                can_consumer_price
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            records[i]['unique_id'],
            records[i]['copyright_year'],
            records[i]['year'],
            records[i]['month'],
            records[i]['purchased_from_studentstore'],
            records[i]['inclusive_access'],
            records[i]['has_used_access_code'],
            records[i]['student_subscription_status'],
            records[i]['product_status'],
            records[i]['account_manager_territory'],
            records[i]['acquisition_source'],
            records[i]['division'],
            records[i]['territory'],
            records[i]['region'],
            records[i]['product_publisher'],
            records[i]['sfdc_state'],
            records[i]['discipline'],
            records[i]['category'],
            records[i]['product_sub_discipline'],
            records[i]['region_territory'],
            records[i]['instructor_email'],
            records[i]['instructor_name'],
            records[i]['profit_center_desc'],
            records[i]['hs_col'],
            records[i]['author_display_name'],
            records[i]['project_title'],
            records[i]['platform'],
            records[i]['school'],
            records[i]['sfdc_country'],
            records[i]['sfdc_city'],
            records[i]['sfdc_account_name'],
            records[i]['paid_unpaid'],
            records[i]['course_name'],
            records[i]['course'],
            records[i]['course_id'],
            records[i]['product_isbn'],
            records[i]['isbn'],
            records[i]['subscription_created_date'],
            records[i]['subs_start_date'],
            records[i]['subs_end_date'],
            records[i]['return_request_date'],
            records[i]['student_subscription_expiration_date'],
            records[i]['conversion_date'],
            records[i]['course_end_date'],
            records[i]['course_start_date'],
            records[i]['course_created_date'],
            records[i]['access_duration'],
            records[i]['access_duration_derived'],
            records[i]['us_net_price'],
            records[i]['us_list_price'],
            records[i]['us_consumer_price'],
            records[i]['can_net_price'],
            records[i]['can_list_price'],
            records[i]['can_consumer_price']
        ))


def create_testing_table(table_name='', data=''):
    """
    Create a testing/dummy table for the SQL queries generated by the model.
    """

    filtered_df = pd.read_csv('macmillan_filtered_md.csv')
    statement = create_table_from_df(filtered_df)
    cur = run_sql_query(statement)
    table_name = 'temp_achive_lp_sapling_combined_activations_ai'

    num_records = 10  # Number of records to generate
    random_data = generate_random_data_for_table(table_name, num_records)  # step 1. generate random data
    
    print("random data is generated")
    insert_random_data(cur, 'temp_achive_lp_sapling_combined_activations_ai', random_data) # step 2. insert random data
    cur.execute("SELECT * FROM temp_achive_lp_sapling_combined_activations_ai;") # example query to check the data
    # print(len(cur.fetchall()))

    return cur

def dump_json(data, filename):
    """
    Dump data as JSON to a file.
    
    :param data: The data to be dumped as JSON
    :param filename: The name of the file to save the JSON data
    """
    with open(filename, 'w') as file:
        json.dump(data, file)

def validate_against_testdb(cur,queries):
    """ executes the queries against the test db and returns the answers (for validation purposes)

    Args:
        cur : cursor object
        queries : list of queries to be validated against test db
    """

    total_count = len(queries)
    valid_count = 0
    cur = create_testing_table()
    for query in queries:
        try:
            cur.execute(query)
            print(cur.fetchall())
            valid_count += 1
        except Exception as e:
            print(e)
            print(f"Error in parsing SQL: {query}")
    
    proportion = (valid_count / total_count)*100
    return proportion
def test_valid_against_db(query, db_name='', validate_non_empty_results=False) -> Tuple[int, str]:
    """
    Test to check that SQL query is valid, empty results considered valid unless validate_non_empty_results=True
    This is for checking on db's from defog_data/defog_data_private only
    Implicitly requires constants.py imported as c from the folder above
    """
    def execute_query(cur, query):
        cur.execute(query)
        return pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])

    if train_dbs is None:

        creds_local_pg = {
                "user": os.getenv("PG_USER"),
                "password": os.getenv("PG_PASSWORD"),
                "host": os.getenv("PG_HOST"),
                "port": os.getenv("PG_PORT"),
            }

        # test patterns
        like_pattern = r"(?<!\\)%"
        escape_percent = r"\\%"

    try:
        if db_name is not '':
            print(" ..step 1: connecting to DB\n")
            conn = psycopg2.connect(
            dbname=db_name,
            user=creds_local_pg["user"],
            password=creds_local_pg["password"],
            host=creds_local_pg["host"],
            port=creds_local_pg["port"],
            )
            cur = conn.cursor()
            escaped_query = re.sub(
                like_pattern, escape_percent, query, flags=re.IGNORECASE
            )  # ignore case of LIKE
            
            print(" ..step 2: sending queries \n")
            results_df = func_timeout(
                20, execute_query, args=(cur, escaped_query)
            )
            print(" ..step 3: results from test db \n")
            print('results from db', results_df)
            cur.close()  # close cursor
            conn.close()  # close connection
            
            if validate_non_empty_results:
                return (1, "-") if len(results_df) > 0 else (0, "Empty results")
            else:
                return (1, "-")
        else:
            print("Database not found")
            return 0.0, "Database not found"


    except Exception as e:  # If unable to run SQL on database, test fails
        if "conn" in locals() or "conn" in globals():
            conn.close()  # close connection if query fails/timeouts
        print("Th error is here", str(e))
        return 0, f"{e.__class__.__name__}: {e}"

def convert_json_to_sql(json_file, db_name,create_table=False):

    with open(json_file, 'r') as f:
        data = json.load(f)
        
    sql_file_content = ""
    for table_name, columns in data["table_metadata"].items():
        sql_file_content += f'CREATE TABLE {table_name} (\n'
        for column in columns:
            if "primary key" in column["column_description"].lower():
                sql_file_content += f'    {column["column_name"]} {column["data_type"]} PRIMARY KEY,\n'
            else:
                sql_file_content += f'    {column["column_name"]} {column["data_type"]},\n'
        sql_file_content = sql_file_content[:-2] + "\n);\n\n"  # Removing the last comma and adding closing parenthesis and semi-colon
    print(" -- ")
    print(sql_file_content)
    with open(f"./defog_data_private/{db_name}.sql", "w") as f:
        f.write(sql_file_content)

    if create_table:
        if train_dbs is None and test_dbs is None:
            # use dummy db for testing (elephantsql)
            creds_local_pg = {
                    "user": os.getenv("PG_USER"),
                    "password": os.getenv("PG_PASSWORD"),
                    "host": os.getenv("PG_HOST"),
                }
            test_dbs = {
                    db_name: db_name,
                }

        conn = psycopg2.connect(
        dbname=db_name,
        user=creds_local_pg["user"],
        password=creds_local_pg["password"],
        host=creds_local_pg["host"],
        )
        cur = conn.cursor()
        cur.execute(sql_file_content)



def combine_json_files(table_metadata_string):
    data = []
    file_list = glob.glob('./qa_collection/qa_pairs*.jsonl')
    for file_path in file_list:
        with jsonlines.open(file_path) as reader:
            data.extend(reader)
    
    delete_qa_files('./qa_collection/') # clean up qa intermediate files 

    print(len(data))
    data = [ d for d in data if type(d) != dict]
    # return data
    questions_cache = []
    for d in data:
        if d[0]['valid'] == 1 and d[0]['question'] not in questions_cache:
            d[0]['instruction'] = d[0]['question']
            d[0]['input'] = ''
            d[0]['output'] = d[0]['sql']
            d[0]['table_metadata_string'] = table_metadata_string
            questions_cache.append(d[0]['question']) # storing the questions to avoid duplicates

            del d[0]['question']
            del d[0]['sql']
            del d[0]['valid']
        else:
            data.remove(d)        
        
    return data

if __name__ == "__main__":
    
    # TEST individual functions as needed

    # csv_to_string_alt('data.csv')
    # read_jsonl('prompt_dict.jsonl')

    # filtered_df = pd.read_csv('macmillan_filtered_md.csv')

    # statement = create_table_from_df(filtered_df)

    convert_json_to_sql('macmillan_md.json', 'ofilbttr', True)

    # run_sql_query(statement)

    # insert_data_into_table()

    # validate_against_testdb()

    # Process the combined data as needed
    # combined_data = combine_json_files()
